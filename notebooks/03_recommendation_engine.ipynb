{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d9175e",
   "metadata": {},
   "source": [
    "# Building the Recommendation Engine\n",
    "\n",
    "## Objective\n",
    "Build a fast, scalable recommendation system using FAISS for similarity search.\n",
    "\n",
    "## What is FAISS?\n",
    "**FAISS (Facebook AI Similarity Search)** is a library for efficient similarity search in high-dimensional spaces.\n",
    "\n",
    "### Why FAISS?\n",
    "- **Fast:** Finds nearest neighbors in milliseconds (even with millions of vectors)\n",
    "- **Scalable:** Used in production by Facebook, Spotify, Pinterest\n",
    "- **Memory efficient:** Various index types for different speed/memory trade-offs\n",
    "\n",
    "### How It Works:\n",
    "1. Build index: Organize 9,280 embeddings for fast lookup\n",
    "2. Search: Given query embedding, find K nearest neighbors\n",
    "3. Return: Top-K most similar papers\n",
    "\n",
    "## Pipeline\n",
    "1. Load embeddings + paper metadata\n",
    "2. Build FAISS index\n",
    "3. Test recommendation quality\n",
    "4. Save index for API deployment\n",
    "\n",
    "## Expected Performance\n",
    "- Index build time: < 1 second\n",
    "- Search time: < 10ms per query\n",
    "- Memory: ~30 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f083a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "FAISS version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"FAISS version: {faiss.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e2aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded embeddings: (9280, 768)\n",
      "✓ Loaded papers: 9280\n",
      "✓ Data verified\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "embeddings = np.load('../data/processed/embeddings.npy')\n",
    "print(f\"✓ Loaded embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Load papers with metadata\n",
    "df = pd.read_pickle('../data/processed/papers_with_embeddings.pkl')\n",
    "print(f\"✓ Loaded papers: {len(df)}\")\n",
    "\n",
    "# Verify they match\n",
    "assert len(embeddings) == len(df), \"Embeddings and dataframe size mismatch!\"\n",
    "print(f\"✓ Data verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc58adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "\n",
      "Building FAISS index...\n",
      "✓ FAISS index built in 0.016 seconds\n",
      "  Index size: 9280 vectors\n"
     ]
    }
   ],
   "source": [
    "# FAISS works with float32 (not float64)\n",
    "embeddings_float32 = embeddings.astype('float32')\n",
    "\n",
    "# Get embedding dimension\n",
    "dimension = embeddings.shape[1]\n",
    "print(f\"Embedding dimension: {dimension}\")\n",
    "\n",
    "# Build FAISS index\n",
    "# IndexFlatL2 = exhaustive search, exact results\n",
    "# (There are faster approximate indexes, but we'll use exact for now)\n",
    "print(\"\\nBuilding FAISS index...\")\n",
    "start_time = time.time()\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_float32)\n",
    "\n",
    "build_time = time.time() - start_time\n",
    "print(f\"✓ FAISS index built in {build_time:.3f} seconds\")\n",
    "print(f\"  Index size: {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2019d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 10 nearest neighbors...\n",
      "✓ Search completed in 161.33 milliseconds\n",
      "\n",
      "Top 10 results:\n",
      "  Indices: [ 100 5331 2250  685 6737  168 3841 1995 7068 8111]\n",
      "  Distances: [ 0.       29.718075 30.052883 30.182102 30.416515 30.983072 32.332436\n",
      " 32.345028 33.084488 33.532707]\n"
     ]
    }
   ],
   "source": [
    "# Test search performance\n",
    "test_idx = 100\n",
    "query_vector = embeddings_float32[test_idx:test_idx+1]  # Keep 2D shape\n",
    "\n",
    "# Search for top 10 most similar papers\n",
    "k = 10  # Number of neighbors to return\n",
    "print(f\"Searching for {k} nearest neighbors...\")\n",
    "\n",
    "start_time = time.time()\n",
    "distances, indices = index.search(query_vector, k)\n",
    "search_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Search completed in {search_time*1000:.2f} milliseconds\")\n",
    "print(f\"\\nTop {k} results:\")\n",
    "print(f\"  Indices: {indices[0]}\")\n",
    "print(f\"  Distances: {distances[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d6761a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recommendation function created\n"
     ]
    }
   ],
   "source": [
    "def recommend_papers(query_idx, k=10, exclude_query=True):\n",
    "    \"\"\"\n",
    "    Find similar papers given a paper index\n",
    "    \n",
    "    Args:\n",
    "        query_idx: Index of the query paper\n",
    "        k: Number of recommendations to return\n",
    "        exclude_query: Whether to exclude the query paper from results\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with recommended papers and their similarity scores\n",
    "    \"\"\"\n",
    "    # Get query vector\n",
    "    query_vector = embeddings_float32[query_idx:query_idx+1]\n",
    "    \n",
    "    # Search (add 1 to k if excluding query)\n",
    "    search_k = k + 1 if exclude_query else k\n",
    "    distances, indices = index.search(query_vector, search_k)\n",
    "    \n",
    "    # Get results\n",
    "    result_indices = indices[0]\n",
    "    result_distances = distances[0]\n",
    "    \n",
    "    # Exclude query if requested\n",
    "    if exclude_query:\n",
    "        result_indices = result_indices[1:]\n",
    "        result_distances = result_distances[1:]\n",
    "    \n",
    "    # Convert L2 distances to similarity scores (0-1 range)\n",
    "    # Smaller distance = higher similarity\n",
    "    # We use: similarity = 1 / (1 + distance)\n",
    "    similarities = 1 / (1 + result_distances)\n",
    "    \n",
    "    # Build result dataframe\n",
    "    results = []\n",
    "    for idx, dist, sim in zip(result_indices, result_distances, similarities):\n",
    "        paper = df.iloc[idx]\n",
    "        results.append({\n",
    "            'paper_id': paper['paper_id'],\n",
    "            'title': paper['title'],\n",
    "            'categories': paper['categories'],\n",
    "            'published': paper['published'],\n",
    "            'similarity': sim,\n",
    "            'distance': dist,\n",
    "            'abstract': paper['abstract'][:200] + '...'  # Truncated\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"✓ Recommendation function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c95d4037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query paper:\n",
      "  Title: BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation\n",
      "  Categories: ['cs.CL', 'cs.AI', 'cs.SE']\n",
      "  Abstract: LLM-as-a-Judge has been widely adopted across various research and practical applications, yet the robustness and reliability of its evaluation remain a critical issue. A core challenge it faces is bi...\n",
      "\n",
      "Top 5 Recommendations:\n",
      "================================================================================\n",
      "\n",
      "1. Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs\n",
      "   Similarity: 0.033 | Distance: 29.72\n",
      "   Categories: ['cs.CV', 'cs.LG']\n",
      "   Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigati...\n",
      "\n",
      "2. Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators\n",
      "   Similarity: 0.032 | Distance: 30.05\n",
      "   Categories: ['cs.LG']\n",
      "   Abstract: In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of ...\n",
      "\n",
      "3. GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?\n",
      "   Similarity: 0.032 | Distance: 30.18\n",
      "   Categories: ['cs.CV', 'cs.AI']\n",
      "   Abstract: The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematica...\n",
      "\n",
      "4. DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation\n",
      "   Similarity: 0.032 | Distance: 30.42\n",
      "   Categories: ['cs.AI', 'cs.IR']\n",
      "   Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scal...\n",
      "\n",
      "5. OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation\n",
      "   Similarity: 0.031 | Distance: 30.98\n",
      "   Categories: ['cs.IR', 'cs.AI']\n",
      "   Abstract: Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity ...\n"
     ]
    }
   ],
   "source": [
    "# Test with a random paper\n",
    "test_idx = 100\n",
    "\n",
    "print(\"Query paper:\")\n",
    "query_paper = df.iloc[test_idx]\n",
    "print(f\"  Title: {query_paper['title']}\")\n",
    "print(f\"  Categories: {query_paper['categories']}\")\n",
    "print(f\"  Abstract: {query_paper['abstract'][:200]}...\\n\")\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = recommend_papers(test_idx, k=5)\n",
    "\n",
    "print(\"Top 5 Recommendations:\")\n",
    "print(\"=\"*80)\n",
    "for i, row in recommendations.iterrows():\n",
    "    print(f\"\\n{i+1}. {row['title']}\")\n",
    "    print(f\"   Similarity: {row['similarity']:.3f} | Distance: {row['distance']:.2f}\")\n",
    "    print(f\"   Categories: {row['categories']}\")\n",
    "    print(f\"   Abstract: {row['abstract']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded6987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.953\n",
      "FAISS L2 distance: 29.72\n"
     ]
    }
   ],
   "source": [
    "# Quick test - compare FAISS distance to actual cosine similarity\n",
    "test_vector = embeddings_float32[100]\n",
    "similar_vector = embeddings_float32[5331]  # Top match from FAISS\n",
    "\n",
    "# Cosine similarity (what we expect)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim = cosine_similarity(\n",
    "    test_vector.reshape(1, -1), \n",
    "    similar_vector.reshape(1, -1)\n",
    ")[0][0]\n",
    "\n",
    "# FAISS L2 distance\n",
    "faiss_dist = 29.72\n",
    "\n",
    "print(f\"Cosine similarity: {cos_sim:.3f}\")\n",
    "print(f\"FAISS L2 distance: {faiss_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75666679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing embeddings...\n",
      "✓ Normalized (sample vector norm: 1.000000)\n",
      "\n",
      "Rebuilding FAISS index with normalized embeddings...\n",
      "✓ Index rebuilt with 9280 normalized vectors\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings to unit length\n",
    "print(\"Normalizing embeddings...\")\n",
    "\n",
    "# Calculate L2 norms (magnitude of each vector)\n",
    "norms = np.linalg.norm(embeddings_float32, axis=1, keepdims=True)\n",
    "embeddings_normalized = embeddings_float32 / norms\n",
    "\n",
    "# Verify normalization\n",
    "sample_norm = np.linalg.norm(embeddings_normalized[0])\n",
    "print(f\"✓ Normalized (sample vector norm: {sample_norm:.6f})\")\n",
    "\n",
    "# Rebuild FAISS index with normalized embeddings\n",
    "print(\"\\nRebuilding FAISS index with normalized embeddings...\")\n",
    "index_normalized = faiss.IndexFlatL2(dimension)\n",
    "index_normalized.add(embeddings_normalized)\n",
    "\n",
    "print(f\"✓ Index rebuilt with {index_normalized.ntotal} normalized vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b44289cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with normalized embeddings:\n",
      "Indices: [ 100 5331 2250  685 6737  168]\n",
      "Distances: [0.         0.09453616 0.09735808 0.09793453 0.09915101 0.10086937]\n",
      "\n",
      "Converted to cosine similarity:\n",
      "0. Paper 100 (query itself)\n",
      "   Distance: 0.000000, Similarity: 1.000000\n",
      "\n",
      "1. Benchmarking Bias Mitigation Toward Fairness Without Harm fr...\n",
      "   Distance: 0.0945, Similarity: 0.9955\n",
      "2. Fault-Tolerant Evaluation for Sample-Efficient Model Perform...\n",
      "   Distance: 0.0974, Similarity: 0.9953\n",
      "3. GenArena: How Can We Achieve Human-Aligned Evaluation for Vi...\n",
      "   Distance: 0.0979, Similarity: 0.9952\n",
      "4. DICE: Discrete Interpretable Comparative Evaluation with Pro...\n",
      "   Distance: 0.0992, Similarity: 0.9951\n",
      "5. OmniReview: A Large-scale Benchmark and LLM-enhanced Framewo...\n",
      "   Distance: 0.1009, Similarity: 0.9949\n"
     ]
    }
   ],
   "source": [
    "# Test search with normalized embeddings\n",
    "query_vector_norm = embeddings_normalized[100:101]\n",
    "distances, indices = index_normalized.search(query_vector_norm, k=6)\n",
    "\n",
    "print(\"Results with normalized embeddings:\")\n",
    "print(f\"Indices: {indices[0]}\")\n",
    "print(f\"Distances: {distances[0]}\\n\")\n",
    "\n",
    "# Now convert to cosine similarity\n",
    "# For normalized vectors: cosine_sim = 1 - (L2_distance² / 2)\n",
    "similarities = 1 - (distances[0] ** 2) / 2\n",
    "\n",
    "print(\"Converted to cosine similarity:\")\n",
    "for i, (idx, dist, sim) in enumerate(zip(indices[0], distances[0], similarities)):\n",
    "    if i == 0:\n",
    "        print(f\"{i}. Paper {idx} (query itself)\")\n",
    "        print(f\"   Distance: {dist:.6f}, Similarity: {sim:.6f}\\n\")\n",
    "    else:\n",
    "        paper = df.iloc[idx]\n",
    "        print(f\"{i}. {paper['title'][:60]}...\")\n",
    "        print(f\"   Distance: {dist:.4f}, Similarity: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3916ad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Updated recommendation function\n"
     ]
    }
   ],
   "source": [
    "def recommend_papers(query_idx, k=10, exclude_query=True):\n",
    "    \"\"\"\n",
    "    Find similar papers given a paper index\n",
    "    \n",
    "    Args:\n",
    "        query_idx: Index of the query paper\n",
    "        k: Number of recommendations to return\n",
    "        exclude_query: Whether to exclude the query paper from results\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with recommended papers and their similarity scores\n",
    "    \"\"\"\n",
    "    # Get normalized query vector\n",
    "    query_vector = embeddings_normalized[query_idx:query_idx+1]\n",
    "    \n",
    "    # Search (add 1 to k if excluding query)\n",
    "    search_k = k + 1 if exclude_query else k\n",
    "    distances, indices = index_normalized.search(query_vector, search_k)\n",
    "    \n",
    "    # Get results\n",
    "    result_indices = indices[0]\n",
    "    result_distances = distances[0]\n",
    "    \n",
    "    # Exclude query if requested\n",
    "    if exclude_query:\n",
    "        result_indices = result_indices[1:]\n",
    "        result_distances = result_distances[1:]\n",
    "    \n",
    "    # Convert L2 distances to cosine similarity\n",
    "    # For normalized vectors: cosine_sim = 1 - (L2_distance² / 2)\n",
    "    similarities = 1 - (result_distances ** 2) / 2\n",
    "    \n",
    "    # Build result dataframe\n",
    "    results = []\n",
    "    for idx, dist, sim in zip(result_indices, result_distances, similarities):\n",
    "        paper = df.iloc[idx]\n",
    "        results.append({\n",
    "            'paper_id': paper['paper_id'],\n",
    "            'title': paper['title'],\n",
    "            'categories': paper['categories'],\n",
    "            'published': paper['published'],\n",
    "            'similarity': sim,\n",
    "            'distance': dist,\n",
    "            'abstract': paper['abstract'][:200] + '...'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"✓ Updated recommendation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eb0fad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query paper:\n",
      "  Title: BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation\n",
      "  Categories: ['cs.CL', 'cs.AI', 'cs.SE']\n",
      "\n",
      "Top 5 Recommendations:\n",
      "================================================================================\n",
      "\n",
      "1. Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs\n",
      "   Similarity: 0.9955\n",
      "   Categories: ['cs.CV', 'cs.LG']\n",
      "   Published: 2026-02-03\n",
      "   Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigati...\n",
      "\n",
      "2. Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators\n",
      "   Similarity: 0.9953\n",
      "   Categories: ['cs.LG']\n",
      "   Published: 2026-02-06\n",
      "   Abstract: In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of ...\n",
      "\n",
      "3. GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?\n",
      "   Similarity: 0.9952\n",
      "   Categories: ['cs.CV', 'cs.AI']\n",
      "   Published: 2026-02-05\n",
      "   Abstract: The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematica...\n",
      "\n",
      "4. DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation\n",
      "   Similarity: 0.9951\n",
      "   Categories: ['cs.AI', 'cs.IR']\n",
      "   Published: 2025-12-27\n",
      "   Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scal...\n",
      "\n",
      "5. OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation\n",
      "   Similarity: 0.9949\n",
      "   Categories: ['cs.IR', 'cs.AI']\n",
      "   Published: 2026-02-09\n",
      "   Abstract: Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity ...\n"
     ]
    }
   ],
   "source": [
    "# Test with the same paper\n",
    "test_idx = 100\n",
    "\n",
    "print(\"Query paper:\")\n",
    "query_paper = df.iloc[test_idx]\n",
    "print(f\"  Title: {query_paper['title']}\")\n",
    "print(f\"  Categories: {query_paper['categories']}\\n\")\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = recommend_papers(test_idx, k=5)\n",
    "\n",
    "print(\"Top 5 Recommendations:\")\n",
    "print(\"=\"*80)\n",
    "for i, row in recommendations.iterrows():\n",
    "    print(f\"\\n{i+1}. {row['title']}\")\n",
    "    print(f\"   Similarity: {row['similarity']:.4f}\")\n",
    "    print(f\"   Categories: {row['categories']}\")\n",
    "    print(f\"   Published: {row['published']}\")\n",
    "    print(f\"   Abstract: {row['abstract']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ad2ea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved normalized embeddings\n",
      "✓ Saved FAISS index\n",
      "✓ Updated metadata\n",
      "\n",
      "✓ All files ready for deployment!\n",
      "  embeddings_normalized.npy: 28.5 MB\n",
      "  papers.index: FAISS index for fast search\n",
      "  papers_with_embeddings.pkl: 20.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Save normalized embeddings\n",
    "np.save('../data/processed/embeddings_normalized.npy', embeddings_normalized)\n",
    "print(\"✓ Saved normalized embeddings\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index_normalized, '../data/processed/papers.index')\n",
    "print(\"✓ Saved FAISS index\")\n",
    "\n",
    "# Update metadata\n",
    "import json\n",
    "metadata = {\n",
    "    'n_papers': len(df),\n",
    "    'embedding_dim': embeddings_normalized.shape[1],\n",
    "    'model': 'allenai/specter2_base',\n",
    "    'normalized': True,\n",
    "    'index_type': 'IndexFlatL2',\n",
    "    'date_generated': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('../data/processed/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"✓ Updated metadata\")\n",
    "\n",
    "print(f\"\\n✓ All files ready for deployment!\")\n",
    "print(f\"  embeddings_normalized.npy: {embeddings_normalized.nbytes / 1e6:.1f} MB\")\n",
    "print(f\"  papers.index: FAISS index for fast search\")\n",
    "print(f\"  papers_with_embeddings.pkl: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f1533",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text-Based Recommendations (Future Feature)\n",
    "\n",
    "Currently, our recommendation system works with **paper-to-paper** similarity:\n",
    "```python\n",
    "recommend_papers(paper_idx=100, k=5)  # Find papers similar to paper #100\n",
    "```\n",
    "\n",
    "### Planned: Text Query Search\n",
    "\n",
    "For production deployment, we'll add **text-to-paper** recommendations:\n",
    "```python\n",
    "# User query\n",
    "\"transformers for natural language processing\"\n",
    "  ↓\n",
    "# SPECTER2 encodes text → embedding\n",
    "  ↓\n",
    "# FAISS finds similar paper embeddings\n",
    "  ↓\n",
    "# Return top K papers\n",
    "```\n",
    "\n",
    "**Implementation:** This will be added in the API (next notebook) where SPECTER2 is loaded once at startup and kept in memory for all requests.\n",
    "\n",
    "**Why not here?** Loading SPECTER2 twice (notebook 2 + notebook 3) exceeds available memory in Jupyter.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
