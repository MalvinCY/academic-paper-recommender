{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48c99b8",
   "metadata": {},
   "source": [
    "# Data Collection from arXiv\n",
    "\n",
    "## Objective\n",
    "Collect academic papers from arXiv API to build our recommendation dataset.\n",
    "\n",
    "## Plan\n",
    "1. Explore arXiv API\n",
    "2. Collect sample papers (~10k for development)\n",
    "3. Extract key fields: title, abstract, authors, categories, citations\n",
    "4. Save to disk for processing\n",
    "\n",
    "## arXiv Categories\n",
    "- cs.AI - Artificial Intelligence\n",
    "- cs.LG - Machine Learning\n",
    "- cs.CL - Computation and Language (NLP)\n",
    "- cs.CV - Computer Vision\n",
    "- stat.ML - Machine Learning (Statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1fa131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd080565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing arXiv API...\n",
      "\n",
      "Sample papers from cs.LG (Machine Learning):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/4c4dh15x491fxl_0xxtx854r0000gn/T/ipykernel_27987/1643493827.py:12: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for i, paper in enumerate(search.results(), 1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Biases in the Blind Spot: Detecting What LLMs Fail to Mention\n",
      "   Authors: Iván Arcuschin, David Chanin, Adrià Garriga-Alonso...\n",
      "   Published: 2026-02-10\n",
      "   Categories: cs.LG, cs.AI\n",
      "   Abstract: Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these ...\n",
      "\n",
      "2. Olaf-World: Orienting Latent Actions for Video World Modeling\n",
      "   Authors: Yuxin Jiang, Yuchao Gu, Ivor W. Tsang...\n",
      "   Published: 2026-02-10\n",
      "   Categories: cs.CV, cs.AI, cs.LG\n",
      "   Abstract: Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfa...\n",
      "\n",
      "3. Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy\n",
      "   Authors: Júlio Oliveira, Rodrigo Ferreira, André Riker...\n",
      "   Published: 2026-02-10\n",
      "   Categories: cs.LG, cs.CR\n",
      "   Abstract: Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, rec...\n",
      "\n",
      "4. Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders\n",
      "   Authors: Amandeep Kumar, Vishal M. Patel...\n",
      "   Published: 2026-02-10\n",
      "   Categories: cs.LG, cs.CV\n",
      "   Abstract: Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transform...\n",
      "\n",
      "5. Step-resolved data attribution for looped transformers\n",
      "   Authors: Georgios Kaissis, David Mildenberger, Juan Felipe Gomez...\n",
      "   Published: 2026-02-10\n",
      "   Categories: cs.LG, cs.AI\n",
      "   Abstract: We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $τ$ recurrent ite...\n",
      "\n",
      "✓ API is working!\n"
     ]
    }
   ],
   "source": [
    "# Test the arXiv API with a simple search\n",
    "print(\"Testing arXiv API...\")\n",
    "\n",
    "# Search for 5 recent ML papers\n",
    "search = arxiv.Search(\n",
    "    query=\"cat:cs.LG\",  # Category: Machine Learning\n",
    "    max_results=5,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "print(\"\\nSample papers from cs.LG (Machine Learning):\\n\")\n",
    "for i, paper in enumerate(search.results(), 1):\n",
    "    print(f\"{i}. {paper.title}\")\n",
    "    print(f\"   Authors: {', '.join([a.name for a in paper.authors[:3]])}...\")\n",
    "    print(f\"   Published: {paper.published.date()}\")\n",
    "    print(f\"   Categories: {', '.join(paper.categories)}\")\n",
    "    print(f\"   Abstract: {paper.summary[:150]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"✓ API is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f66378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/4c4dh15x491fxl_0xxtx854r0000gn/T/ipykernel_27987/3173974016.py:3: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  paper = next(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available paper attributes:\n",
      "\n",
      "Title: Design Rule Checking with a CNN Based Feature Extractor\n",
      "Paper ID: http://arxiv.org/abs/2012.11510v1\n",
      "Published: 2020-12-21 17:26:31+00:00\n",
      "Updated: 2020-12-21 17:26:31+00:00\n",
      "Authors: ['Luis Francisco', 'Tanmay Lagare', 'Arpit Jain', 'Somal Chaudhary', 'Madhura Kulkarni', 'Divya Sardana', 'W. Rhett Davis', 'Paul Franzon']\n",
      "Categories: ['cs.LG']\n",
      "Primary Category: cs.LG\n",
      "Abstract length: 700 characters\n",
      "PDF URL: https://arxiv.org/pdf/2012.11510v1\n",
      "\n",
      "Full abstract:\n",
      "Design rule checking (DRC) is getting increasingly complex in advanced nodes technologies. It would be highly desirable to have a fast interactive DRC engine that could be used during layout. In this work, we establish the proof of feasibility for such an engine. The proposed model consists of a convolutional neural network (CNN) trained to detect DRC violations. The model was trained with artificial data that was derived from a set of $50$ SRAM designs. The focus in this demonstration was metal 1 rules. Using this solution, we can detect multiple DRC violations 32x faster than Boolean checkers with an accuracy of up to 92. The proposed solution can be easily expanded to a complete rule set.\n"
     ]
    }
   ],
   "source": [
    "# Get one paper and examine all available fields\n",
    "search = arxiv.Search(query=\"cat:cs.LG\", max_results=1)\n",
    "paper = next(search.results())\n",
    "\n",
    "print(\"Available paper attributes:\\n\")\n",
    "print(f\"Title: {paper.title}\")\n",
    "print(f\"Paper ID: {paper.entry_id}\")\n",
    "print(f\"Published: {paper.published}\")\n",
    "print(f\"Updated: {paper.updated}\")\n",
    "print(f\"Authors: {[a.name for a in paper.authors]}\")\n",
    "print(f\"Categories: {paper.categories}\")\n",
    "print(f\"Primary Category: {paper.primary_category}\")\n",
    "print(f\"Abstract length: {len(paper.summary)} characters\")\n",
    "print(f\"PDF URL: {paper.pdf_url}\")\n",
    "print(f\"\\nFull abstract:\\n{paper.summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86e77ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target collection:\n",
      "  Categories: 6\n",
      "  Papers per category: 2000\n",
      "  Total papers: 12000\n",
      "\n",
      "Estimated time: ~10-15 minutes\n"
     ]
    }
   ],
   "source": [
    "# Categories we want to collect from\n",
    "CATEGORIES = [\n",
    "    'cs.AI',      # Artificial Intelligence\n",
    "    'cs.LG',      # Machine Learning\n",
    "    'cs.CL',      # Computation and Language (NLP)\n",
    "    'cs.CV',      # Computer Vision\n",
    "    'cs.IR',      # Information Retrieval\n",
    "    'stat.ML'     # Statistics - Machine Learning\n",
    "]\n",
    "\n",
    "# How many papers per category\n",
    "PAPERS_PER_CATEGORY = 2000  # Start with 2k per category = 12k total\n",
    "\n",
    "print(f\"Target collection:\")\n",
    "print(f\"  Categories: {len(CATEGORIES)}\")\n",
    "print(f\"  Papers per category: {PAPERS_PER_CATEGORY}\")\n",
    "print(f\"  Total papers: {len(CATEGORIES) * PAPERS_PER_CATEGORY}\")\n",
    "print(f\"\\nEstimated time: ~10-15 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17063449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing balanced collection with cs.AI (10 papers)...\n",
      "  Collecting recent papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  recent: 100%|██████████| 5/5 [00:00<00:00, 23.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collecting relevant papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  relevant: 100%|██████████| 5/5 [00:03<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Collected 10 unique papers\n",
      "Date range: 2020-12-09 → 2026-02-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the new Client API (fixes deprecation warning)\n",
    "client = arxiv.Client()\n",
    "\n",
    "def collect_papers_balanced(category, max_results=1000):\n",
    "    \"\"\"\n",
    "    Collect papers using both recency and relevance\n",
    "    to ensure balanced coverage across time periods\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    seen_ids = set()\n",
    "    \n",
    "    # Sort criteria to use\n",
    "    sort_criteria = [\n",
    "        (arxiv.SortCriterion.SubmittedDate, \"recent\"),\n",
    "        (arxiv.SortCriterion.Relevance, \"relevant\")\n",
    "    ]\n",
    "    \n",
    "    for sort_by, label in sort_criteria:\n",
    "        print(f\"  Collecting {label} papers...\")\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query=f\"cat:{category}\",\n",
    "            max_results=max_results,\n",
    "            sort_by=sort_by\n",
    "        )\n",
    "        \n",
    "        for paper in tqdm(client.results(search),\n",
    "                         total=max_results,\n",
    "                         desc=f\"  {label}\"):\n",
    "            paper_id = paper.entry_id.split('/')[-1]\n",
    "            \n",
    "            # Skip if already collected\n",
    "            if paper_id in seen_ids:\n",
    "                continue\n",
    "            \n",
    "            seen_ids.add(paper_id)\n",
    "            papers.append({\n",
    "                'paper_id': paper_id,\n",
    "                'title': paper.title,\n",
    "                'abstract': paper.summary,\n",
    "                'authors': [a.name for a in paper.authors],\n",
    "                'categories': paper.categories,\n",
    "                'primary_category': paper.primary_category,\n",
    "                'published': str(paper.published.date()),\n",
    "                'updated': str(paper.updated.date()),\n",
    "                'pdf_url': paper.pdf_url\n",
    "            })\n",
    "    \n",
    "    return papers\n",
    "\n",
    "# Test with small sample\n",
    "print(\"Testing balanced collection with cs.AI (10 papers)...\")\n",
    "test_papers = collect_papers_balanced('cs.AI', max_results=5)\n",
    "print(f\"\\n✓ Collected {len(test_papers)} unique papers\")\n",
    "\n",
    "# Check date range\n",
    "dates = [p['published'] for p in test_papers]\n",
    "print(f\"Date range: {min(dates)} → {max(dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f372bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting balanced data collection...\n",
      "==================================================\n",
      "\n",
      "Collecting cs.AI...\n",
      "  Collecting recent papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  recent: 100%|██████████| 1000/1000 [00:27<00:00, 35.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collecting relevant papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  relevant: 100%|██████████| 1000/1000 [00:38<00:00, 26.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collected 2000 unique papers from cs.AI\n",
      "  Total so far: 2000\n",
      "\n",
      "Collecting cs.LG...\n",
      "  Collecting recent papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  recent: 100%|██████████| 1000/1000 [00:28<00:00, 35.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collecting relevant papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  relevant: 100%|██████████| 1000/1000 [00:40<00:00, 24.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collected 2000 unique papers from cs.LG\n",
      "  Total so far: 4000\n",
      "\n",
      "Collecting cs.CL...\n",
      "  Collecting recent papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  recent: 100%|██████████| 1000/1000 [00:28<00:00, 35.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collecting relevant papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  relevant: 100%|██████████| 1000/1000 [00:42<00:00, 23.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collected 2000 unique papers from cs.CL\n",
      "  Total so far: 6000\n",
      "\n",
      "Collecting cs.CV...\n",
      "  Collecting recent papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  recent: 100%|██████████| 1000/1000 [00:39<00:00, 25.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collecting relevant papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  relevant: 100%|██████████| 1000/1000 [00:36<00:00, 27.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collected 2000 unique papers from cs.CV\n",
      "  Total so far: 8000\n",
      "\n",
      "Collecting cs.IR...\n",
      "  Collecting recent papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  recent: 100%|██████████| 1000/1000 [00:36<00:00, 27.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collecting relevant papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  relevant: 100%|██████████| 1000/1000 [00:54<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collected 2000 unique papers from cs.IR\n",
      "  Total so far: 10000\n",
      "\n",
      "Collecting stat.ML...\n",
      "  Collecting recent papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  recent: 100%|██████████| 1000/1000 [00:43<00:00, 23.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collecting relevant papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  relevant: 100%|██████████| 1000/1000 [01:33<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collected 2000 unique papers from stat.ML\n",
      "  Total so far: 12000\n",
      "\n",
      "==================================================\n",
      "✓ Collection complete!\n",
      "  Total papers (before global dedup): 12000\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = [\n",
    "    'cs.AI',\n",
    "    'cs.LG',\n",
    "    'cs.CL',\n",
    "    'cs.CV',\n",
    "    'cs.IR',\n",
    "    'stat.ML'\n",
    "]\n",
    "\n",
    "PAPERS_PER_SORT = 1000  # 1000 recent + 1000 relevant per category\n",
    "\n",
    "all_papers = []\n",
    "\n",
    "print(\"Starting balanced data collection...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    print(f\"\\nCollecting {category}...\")\n",
    "    \n",
    "    papers = collect_papers_balanced(category, max_results=PAPERS_PER_SORT)\n",
    "    all_papers.extend(papers)\n",
    "    \n",
    "    print(f\"✓ Collected {len(papers)} unique papers from {category}\")\n",
    "    print(f\"  Total so far: {len(all_papers)}\")\n",
    "    \n",
    "    # Be polite to the API\n",
    "    time.sleep(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"✓ Collection complete!\")\n",
    "print(f\"  Total papers (before global dedup): {len(all_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cbc88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deduplication:\n",
      "  Total papers: 12000\n",
      "\n",
      "After deduplication:\n",
      "  Unique papers: 9562\n",
      "  Duplicates removed: 2438\n",
      "\n",
      "Dataset overview:\n",
      "  Date range: 2008-08-07 → 2026-02-10\n",
      "\n",
      "Papers per primary category:\n",
      "primary_category\n",
      "cs.LG          2036\n",
      "cs.CV          1676\n",
      "cs.CL          1652\n",
      "cs.IR          1264\n",
      "cs.AI           650\n",
      "               ... \n",
      "cs.PF             1\n",
      "q-bio.MN          1\n",
      "nlin.CD           1\n",
      "cs.GL             1\n",
      "astro-ph.HE       1\n",
      "Name: count, Length: 102, dtype: int64\n",
      "\n",
      "✓ Saved to ../data/raw/arxiv_papers.csv\n",
      "✓ Saved to ../data/raw/arxiv_papers.json\n"
     ]
    }
   ],
   "source": [
    "# Convert to dataframe\n",
    "df = pd.DataFrame(all_papers)\n",
    "\n",
    "print(\"Before deduplication:\")\n",
    "print(f\"  Total papers: {len(df)}\")\n",
    "\n",
    "# Remove duplicates based on paper_id\n",
    "df = df.drop_duplicates(subset='paper_id')\n",
    "\n",
    "print(f\"\\nAfter deduplication:\")\n",
    "print(f\"  Unique papers: {len(df)}\")\n",
    "print(f\"  Duplicates removed: {len(all_papers) - len(df)}\")\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Basic stats\n",
    "print(f\"\\nDataset overview:\")\n",
    "print(f\"  Date range: {df['published'].min()} → {df['published'].max()}\")\n",
    "print(f\"\\nPapers per primary category:\")\n",
    "print(df['primary_category'].value_counts())\n",
    "\n",
    "# Save to disk\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "df.to_csv('../data/raw/arxiv_papers.csv', index=False)\n",
    "print(f\"\\n✓ Saved to ../data/raw/arxiv_papers.csv\")\n",
    "\n",
    "df.to_json('../data/raw/arxiv_papers.json', orient='records', indent=2)\n",
    "print(f\"✓ Saved to ../data/raw/arxiv_papers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9913eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract Statistics:\n",
      "  Mean length:   1228 characters\n",
      "  Median length: 1230 characters\n",
      "  Min length:    125 characters\n",
      "  Max length:    1924 characters\n",
      "\n",
      "Papers per year (last 5 years):\n",
      "year\n",
      "2022     118\n",
      "2023     276\n",
      "2024     152\n",
      "2025     782\n",
      "2026    4133\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing abstracts: 0\n",
      "Empty abstracts: 0\n",
      "\n",
      "✓ EDA complete - data looks clean!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "# Abstract length distribution\n",
    "df['abstract_length'] = df['abstract'].str.len()\n",
    "\n",
    "print(\"Abstract Statistics:\")\n",
    "print(f\"  Mean length:   {df['abstract_length'].mean():.0f} characters\")\n",
    "print(f\"  Median length: {df['abstract_length'].median():.0f} characters\")\n",
    "print(f\"  Min length:    {df['abstract_length'].min()} characters\")\n",
    "print(f\"  Max length:    {df['abstract_length'].max()} characters\")\n",
    "\n",
    "# Papers per year\n",
    "df['year'] = pd.to_datetime(df['published']).dt.year\n",
    "yearly_counts = df['year'].value_counts().sort_index()\n",
    "\n",
    "print(f\"\\nPapers per year (last 5 years):\")\n",
    "print(yearly_counts.tail(5))\n",
    "\n",
    "# Check for missing abstracts\n",
    "missing = df['abstract'].isna().sum()\n",
    "print(f\"\\nMissing abstracts: {missing}\")\n",
    "print(f\"Empty abstracts: {(df['abstract'] == '').sum()}\")\n",
    "\n",
    "print(\"\\n✓ EDA complete - data looks clean!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d07b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest abstracts:\n",
      "\n",
      "Title: Anomaly Detection Based on Deep Learning Using Video for Prevention of Industrial Accidents\n",
      "Length: 125 chars\n",
      "Abstract: This paper proposes an anomaly detection method for the prevention of industrial accidents using machine learning technology.\n",
      "--------------------------------------------------\n",
      "\n",
      "Title: Covapixels\n",
      "Length: 167 chars\n",
      "Abstract: We propose and discuss the summarization of superpixel-type image tiles/patches using mean and covariance information. We refer to the resulting objects as covapixels.\n",
      "--------------------------------------------------\n",
      "\n",
      "Title: A Minesweeper Solver Using Logic Inference, CSP and Sampling\n",
      "Length: 187 chars\n",
      "Abstract: Minesweeper as a puzzle video game and is proved that it is an NPC problem. We use CSP, Logic Inference and Sampling to make a minesweeper solver and we limit us each select in 5 seconds.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check the suspiciously short abstracts\n",
    "print(\"Shortest abstracts:\")\n",
    "short_papers = df.nsmallest(3, 'abstract_length')[['title', 'abstract', 'abstract_length']]\n",
    "\n",
    "for _, row in short_papers.iterrows():\n",
    "    print(f\"\\nTitle: {row['title']}\")\n",
    "    print(f\"Length: {row['abstract_length']} chars\")\n",
    "    print(f\"Abstract: {row['abstract']}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ba4b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impact of different minimum abstract lengths:\n",
      "\n",
      "Threshold    Papers Kept     Papers Removed  % Removed \n",
      "----------------------------------------------------\n",
      "200          9557            5               0.1%\n",
      "400          9516            46              0.5%\n",
      "600          9280            282             2.9%\n",
      "800          8601            961             10.1%\n",
      "1000         7181            2381            24.9%\n",
      "\n",
      "Current dataset: 9562 papers\n"
     ]
    }
   ],
   "source": [
    "# Check impact of different thresholds\n",
    "thresholds = [200, 400, 600, 800, 1000]\n",
    "\n",
    "print(\"Impact of different minimum abstract lengths:\\n\")\n",
    "print(f\"{'Threshold':<12} {'Papers Kept':<15} {'Papers Removed':<15} {'% Removed':<10}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    kept = len(df[df['abstract_length'] >= threshold])\n",
    "    removed = len(df) - kept\n",
    "    pct = (removed / len(df)) * 100\n",
    "    print(f\"{threshold:<12} {kept:<15} {removed:<15} {pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nCurrent dataset: {len(df)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe27960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers between 600-800 chars: 679\n",
      "\n",
      "Sample abstracts in this range:\n",
      "\n",
      "Title: How important is Recall for Measuring Retrieval Quality?\n",
      "Length: 680 chars\n",
      "Abstract: In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.\n",
      "--------------------------------------------------\n",
      "Title: Demonstrating PAR4SEM - A Semantic Writing Aid with Adaptive Paraphrasing\n",
      "Length: 674 chars\n",
      "Abstract: In this paper, we present Par4Sem, a semantic writing aid tool based on adaptive paraphrasing. Unlike many annotation tools that are primarily used to collect training examples, Par4Sem is integrated into a real word application, in this case a writing aid tool, in order to collect training examples from usage data. Par4Sem is a tool, which supports an adaptive, iterative, and interactive process where the underlying machine learning models are updated for each iteration using new training examples from usage data. After motivating the use of ever-learning tools in NLP applications, we evaluate Par4Sem by adopting it to a text simplification task through mere usage.\n",
      "--------------------------------------------------\n",
      "Title: Object-Oriented Transition Modeling with Inductive Logic Programming\n",
      "Length: 684 chars\n",
      "Abstract: Building models of the world from observation, i.e., induction, is one of the major challenges in machine learning. In order to be useful, models need to maintain accuracy when used in novel situations, i.e., generalize. In addition, they should be easy to interpret and efficient to train. Prior work has investigated these concepts in the context of object-oriented representations inspired by human cognition. In this paper, we develop a novel learning algorithm that is substantially more powerful than these previous methods. Our thorough experiments, including ablation tests and comparison with neural baselines, demonstrate a significant improvement over the state-of-the-art.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Look at papers between 600-800 characters\n",
    "mid_range = df[(df['abstract_length'] >= 600) & \n",
    "               (df['abstract_length'] < 800)]\n",
    "\n",
    "print(f\"Papers between 600-800 chars: {len(mid_range)}\")\n",
    "print(\"\\nSample abstracts in this range:\\n\")\n",
    "\n",
    "for _, row in mid_range.sample(3).iterrows():\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Length: {row['abstract_length']} chars\")\n",
    "    print(f\"Abstract: {row['abstract']}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93967e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 9562 papers\n",
      "After filtering:  9280 papers\n",
      "Removed:          282 papers\n",
      "\n",
      "✓ Dataset ready for embedding generation!\n",
      "  Papers: 9280\n",
      "  Date range: 2008-08-07 → 2026-02-10\n",
      "  Avg abstract length: 1251 chars\n",
      "\n",
      "✓ Saved cleaned dataset\n"
     ]
    }
   ],
   "source": [
    "# Filter short abstracts\n",
    "original_count = len(df)\n",
    "df = df[df['abstract_length'] >= 600].reset_index(drop=True)\n",
    "\n",
    "print(f\"Before filtering: {original_count} papers\")\n",
    "print(f\"After filtering:  {len(df)} papers\")\n",
    "print(f\"Removed:          {original_count - len(df)} papers\")\n",
    "print(f\"\\n✓ Dataset ready for embedding generation!\")\n",
    "print(f\"  Papers: {len(df)}\")\n",
    "print(f\"  Date range: {df['published'].min()} → {df['published'].max()}\")\n",
    "print(f\"  Avg abstract length: {df['abstract_length'].mean():.0f} chars\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv('../data/raw/arxiv_papers_cleaned.csv', index=False)\n",
    "df.to_json('../data/raw/arxiv_papers_cleaned.json', orient='records', indent=2)\n",
    "print(f\"\\n✓ Saved cleaned dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
