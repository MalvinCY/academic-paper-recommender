{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db49d91",
   "metadata": {},
   "source": [
    "# Embedding Generation with SciBERT\n",
    "\n",
    "## Objective\n",
    "Convert paper abstracts into dense vector representations (embeddings) \n",
    "using SciBERT, a BERT model pre-trained on scientific text.\n",
    "\n",
    "## Why SciBERT?\n",
    "- Pre-trained on 1.14M scientific papers (vs Wikipedia for regular BERT)\n",
    "- Understands domain-specific vocabulary (ML, physics, biology)\n",
    "- Better semantic similarity for scientific concepts\n",
    "- Used in production by Semantic Scholar, Allen AI\n",
    "\n",
    "## Pipeline\n",
    "1. Load cleaned paper dataset (9,280 papers)\n",
    "2. Load pre-trained SciBERT model\n",
    "3. Generate embeddings for all abstracts\n",
    "4. Save embeddings + FAISS index for fast similarity search\n",
    "\n",
    "## Output\n",
    "- embeddings.npy: 9,280 × 768 matrix (~27MB)\n",
    "- papers.index: FAISS index for fast similarity search\n",
    "- papers_with_embeddings.pkl: Papers + embeddings combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36d532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malvinsiew/PycharmProjects/Datascience/academic-paper-recommender/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "PyTorch version: 2.0.1\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07955bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded dataset\n",
      "  Papers: 9280\n",
      "  Columns: ['paper_id', 'title', 'abstract', 'authors', 'categories', 'primary_category', 'published', 'updated', 'pdf_url', 'abstract_length', 'year']\n",
      "\n",
      "Sample abstract:\n",
      "Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefine...\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned dataset\n",
    "df = pd.read_csv('../data/raw/arxiv_papers_cleaned.csv')\n",
    "\n",
    "print(f\"✓ Loaded dataset\")\n",
    "print(f\"  Papers: {len(df)}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "print(f\"\\nSample abstract:\")\n",
    "print(df['abstract'].iloc[0][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159eba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SciBERT model...\n",
      "(First time will download ~440MB model weights)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SciBERT loaded!\n",
      "  Embedding dimension: 768\n",
      "  Max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "# Load SciBERT model\n",
    "# 'allenai/scibert_scivocab_uncased' is the official SciBERT model\n",
    "# sentence-transformers wraps it for easy embedding generation\n",
    "\n",
    "print(\"Loading SciBERT model...\")\n",
    "print(\"(First time will download ~440MB model weights)\\n\")\n",
    "\n",
    "model = SentenceTransformer('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "print(f\"✓ SciBERT loaded!\")\n",
    "print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  Max sequence length: {model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6743a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/specter2_base. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SPECTER2 model...\n",
      "(First time will download ~440MB model weights)\n",
      "\n",
      "✓ SPECTER2 loaded!\n",
      "  Embedding dimension: 768\n",
      "  Max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "# Load SPECTER2 instead\n",
    "print(\"Loading SPECTER2 model...\")\n",
    "print(\"(First time will download ~440MB model weights)\\n\")\n",
    "\n",
    "model = SentenceTransformer('allenai/specter2_base')\n",
    "\n",
    "print(f\"✓ SPECTER2 loaded!\")\n",
    "print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  Max sequence length: {model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58001009",
   "metadata": {},
   "source": [
    "\"I initially planned to use SciBERT, but switched to SPECTER2 because it was specifically trained on citation relationships between papers - meaning papers that cite each other have similar embeddings. This directly aligns with our recommendation goal: if researchers cite papers together, they're likely related.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a21e2e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 9,280 papers...\n",
      "(This will take ~5-10 minutes)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 290/290 [45:32<00:00,  9.42s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Embeddings generated!\n",
      "  Shape: (9280, 768)\n",
      "  Size in memory: 28.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all abstracts\n",
    "print(\"Generating embeddings for 9,280 papers...\")\n",
    "print(\"(This will take ~5-10 minutes)\\n\")\n",
    "\n",
    "# Convert abstracts to list\n",
    "abstracts = df['abstract'].tolist()\n",
    "\n",
    "# Generate embeddings in batches (faster + more memory efficient)\n",
    "# batch_size=32 is a good default\n",
    "embeddings = model.encode(\n",
    "    abstracts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Embeddings generated!\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Size in memory: {embeddings.nbytes / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce239f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test paper:\n",
      "  Title: BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation\n",
      "  Categories: ['cs.CL', 'cs.AI', 'cs.SE']\n",
      "  Abstract: LLM-as-a-Judge has been widely adopted across various research and practical applications, yet the robustness and reliability of its evaluation remain a critical issue. A core challenge it faces is bi...\n",
      "\n",
      "Top 5 most similar papers:\n",
      "\n",
      "1. Similarity: 0.953\n",
      "   Title: Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs\n",
      "   Categories: ['cs.CV', 'cs.LG']\n",
      "\n",
      "2. Similarity: 0.951\n",
      "   Title: Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators\n",
      "   Categories: ['cs.LG']\n",
      "\n",
      "3. Similarity: 0.951\n",
      "   Title: GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?\n",
      "   Categories: ['cs.CV', 'cs.AI']\n",
      "\n",
      "4. Similarity: 0.950\n",
      "   Title: DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation\n",
      "   Categories: ['cs.AI', 'cs.IR']\n",
      "\n",
      "5. Similarity: 0.950\n",
      "   Title: OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation\n",
      "   Categories: ['cs.IR', 'cs.AI']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test: Find similar papers to a sample paper\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Pick a random paper\n",
    "test_idx = 100\n",
    "test_paper = df.iloc[test_idx]\n",
    "\n",
    "print(\"Test paper:\")\n",
    "print(f\"  Title: {test_paper['title']}\")\n",
    "print(f\"  Categories: {test_paper['categories']}\")\n",
    "print(f\"  Abstract: {test_paper['abstract'][:200]}...\\n\")\n",
    "\n",
    "# Calculate similarity to all other papers\n",
    "test_embedding = embeddings[test_idx].reshape(1, -1)\n",
    "similarities = cosine_similarity(test_embedding, embeddings)[0]\n",
    "\n",
    "# Get top 5 most similar papers (excluding itself)\n",
    "top_indices = np.argsort(similarities)[::-1][1:6]  # Skip index 0 (itself)\n",
    "\n",
    "print(\"Top 5 most similar papers:\\n\")\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    sim = similarities[idx]\n",
    "    paper = df.iloc[idx]\n",
    "    print(f\"{rank}. Similarity: {sim:.3f}\")\n",
    "    print(f\"   Title: {paper['title']}\")\n",
    "    print(f\"   Categories: {paper['categories']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf75e908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test paper (Computer Vision):\n",
      "  Title: Olaf-World: Orienting Latent Actions for Video World Modeling\n",
      "  Categories: ['cs.CV', 'cs.AI', 'cs.LG']\n",
      "\n",
      "Top 5 similar papers:\n",
      "\n",
      "1. Similarity: 0.957\n",
      "   Title: VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model\n",
      "   Categories: ['cs.RO', 'cs.CV']\n",
      "\n",
      "2. Similarity: 0.957\n",
      "   Title: BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks\n",
      "   Categories: ['cs.RO', 'cs.CV']\n",
      "\n",
      "3. Similarity: 0.954\n",
      "   Title: Segment to Focus: Guiding Latent Action Models in the Presence of Distractors\n",
      "   Categories: ['cs.LG', 'cs.CV']\n",
      "\n",
      "4. Similarity: 0.952\n",
      "   Title: MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction\n",
      "   Categories: ['cs.RO', 'cs.CV']\n",
      "\n",
      "5. Similarity: 0.951\n",
      "   Title: VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models\n",
      "   Categories: ['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find a computer vision paper\n",
    "cv_papers = df[df['primary_category'] == 'cs.CV']\n",
    "test_idx_cv = cv_papers.index[0]\n",
    "\n",
    "test_paper = df.iloc[test_idx_cv]\n",
    "print(\"Test paper (Computer Vision):\")\n",
    "print(f\"  Title: {test_paper['title']}\")\n",
    "print(f\"  Categories: {test_paper['categories']}\\n\")\n",
    "\n",
    "# Find similar papers\n",
    "test_embedding = embeddings[test_idx_cv].reshape(1, -1)\n",
    "similarities = cosine_similarity(test_embedding, embeddings)[0]\n",
    "top_indices = np.argsort(similarities)[::-1][1:6]\n",
    "\n",
    "print(\"Top 5 similar papers:\\n\")\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    sim = similarities[idx]\n",
    "    paper = df.iloc[idx]\n",
    "    print(f\"{rank}. Similarity: {sim:.3f}\")\n",
    "    print(f\"   Title: {paper['title']}\")\n",
    "    print(f\"   Categories: {paper['categories']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5964b5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved embeddings.npy\n",
      "✓ Saved papers_with_embeddings.pkl\n",
      "✓ Saved metadata.json\n",
      "\n",
      "✓ All files saved to ../data/processed/\n",
      "  Total size: 49.4 MB\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Create directory\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save embeddings as numpy array\n",
    "np.save('../data/processed/embeddings.npy', embeddings)\n",
    "print(\"✓ Saved embeddings.npy\")\n",
    "\n",
    "# Save papers dataframe with embeddings\n",
    "df['embedding'] = list(embeddings)\n",
    "df.to_pickle('../data/processed/papers_with_embeddings.pkl')\n",
    "print(\"✓ Saved papers_with_embeddings.pkl\")\n",
    "\n",
    "# Save metadata (for quick reference)\n",
    "metadata = {\n",
    "    'n_papers': len(df),\n",
    "    'embedding_dim': embeddings.shape[1],\n",
    "    'model': 'allenai/specter2_base',\n",
    "    'date_generated': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('../data/processed/metadata.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"✓ Saved metadata.json\")\n",
    "\n",
    "print(f\"\\n✓ All files saved to ../data/processed/\")\n",
    "print(f\"  Total size: {(embeddings.nbytes + df.memory_usage(deep=True).sum()) / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833016c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
